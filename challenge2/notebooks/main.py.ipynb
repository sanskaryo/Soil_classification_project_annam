{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "369a99cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T11:20:03.264507Z",
     "iopub.status.busy": "2025-05-23T11:20:03.264281Z",
     "iopub.status.idle": "2025-05-23T11:20:14.316192Z",
     "shell.execute_reply": "2025-05-23T11:20:14.315454Z",
     "shell.execute_reply.started": "2025-05-23T11:20:03.264482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# PyTorch and vision tools\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "from PIL import Image\n",
    "\n",
    "# Sklearn tools for evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187cda1-bd7c-4ab8-b399-625a6a321a89",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Training Data\n",
    "\n",
    "- Loaded the CSV file containing the training labels.\n",
    "- Created a label mapping to convert soil type names into numeric labels for modeling.\n",
    "- Defined the directory path containing training images.\n",
    "- Since image filenames may have different extensions (`.jpg`, `.jpeg`, `.png`), implemented a function to correctly resolve the actual filename for each image.\n",
    "- Applied this function to map each image ID to its resolved filename.\n",
    "- Removed any rows where the image file could not be found to ensure data consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6e92145-0cdf-49f7-b543-20d9654caa5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T11:23:37.578695Z",
     "iopub.status.busy": "2025-05-23T11:23:37.578403Z",
     "iopub.status.idle": "2025-05-23T11:23:37.635319Z",
     "shell.execute_reply": "2025-05-23T11:23:37.634588Z",
     "shell.execute_reply.started": "2025-05-23T11:23:37.578673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the training labels CSV\n",
    "train_df = pd.read_csv('/kaggle/input/soil-classification/soil_classification-2025/train_labels.csv')\n",
    "\n",
    "# Map soil type strings to numeric labels\n",
    "label_map = {\n",
    "    'Alluvial soil': 0,\n",
    "    'Black Soil': 1,\n",
    "    'Clay soil': 2,\n",
    "    'Red soil': 3\n",
    "}\n",
    "train_df['label'] = train_df['soil_type'].map(label_map)\n",
    "\n",
    "# Define the training image directory\n",
    "train_dir = '/kaggle/input/soil-classification/soil_classification-2025/train'\n",
    "\n",
    "# Resolve actual image filenames (handles .jpg/.jpeg/.png extensions)\n",
    "train_files = os.listdir(train_dir)\n",
    "train_files_lower = {f.lower(): f for f in train_files}\n",
    "\n",
    "def resolve_image_file(image_id):\n",
    "    base = os.path.splitext(image_id)[0]\n",
    "    for ext in ['.jpg', '.jpeg', '.png']:\n",
    "        fname = f\"{base}{ext}\"\n",
    "        if fname.lower() in train_files_lower:\n",
    "            return train_files_lower[fname.lower()]\n",
    "    return None\n",
    "\n",
    "# Apply filename resolution\n",
    "train_df['resolved_image'] = train_df['image_id'].apply(resolve_image_file)\n",
    "train_df = train_df.dropna(subset=['resolved_image']).reset_index(drop=True)\n",
    "train_df['image_id'] = train_df['resolved_image']\n",
    "train_df.drop(columns=['resolved_image'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04343463-ad38-4237-b52a-636082866d00",
   "metadata": {},
   "source": [
    "## Data Transformation and Custom Dataset\n",
    "\n",
    "- Set the target image size to 224x224 pixels.\n",
    "- Defined normalization parameters matching pretrained model expectations.\n",
    "- Created data augmentation pipeline for training, including resizing, random horizontal flips, rotations, and color jitter to improve model robustness.\n",
    "- Defined a simpler transformation for validation and testing without augmentation.\n",
    "- Implemented a custom `SoilDataset` class to:\n",
    "  - Load images from disk.\n",
    "  - Apply transformations.\n",
    "  - Return image-label pairs for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d115466-59f5-47d8-815d-f627057f1932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T11:23:51.504928Z",
     "iopub.status.busy": "2025-05-23T11:23:51.504155Z",
     "iopub.status.idle": "2025-05-23T11:23:51.512257Z",
     "shell.execute_reply": "2025-05-23T11:23:51.511534Z",
     "shell.execute_reply.started": "2025-05-23T11:23:51.504892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Image size and normalization\n",
    "IMG_SIZE = 224\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Augmentation for training\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Minimal transform for validation/test\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Custom dataset class for loading soil images\n",
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = os.path.join(self.img_dir, row.image_id)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        label = row.label\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44304830-0397-4849-8bbe-93aa5d88077d",
   "metadata": {},
   "source": [
    "## Train-Validation Split and Model Setup\n",
    "\n",
    "- Split the data into training and validation sets with stratification to maintain label distribution.\n",
    "- Created custom dataset objects and corresponding dataloaders with batch size 32.\n",
    "- Loaded a pretrained ResNet34 model and replaced its final fully connected layer to output predictions for 4 soil classes.\n",
    "- Set up the device to use GPU if available for faster training.\n",
    "- Defined the loss function as Cross-Entropy Loss suitable for multi-class classification.\n",
    "- Used the Adam optimizer with a learning rate of 0.0001.\n",
    "- Added a learning rate scheduler to reduce the learning rate by half every 5 epochs to improve convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbfc26b-89cd-4b3b-8fb2-b207d7762f43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T11:23:56.563737Z",
     "iopub.status.busy": "2025-05-23T11:23:56.563486Z",
     "iopub.status.idle": "2025-05-23T11:23:57.808010Z",
     "shell.execute_reply": "2025-05-23T11:23:57.807232Z",
     "shell.execute_reply.started": "2025-05-23T11:23:56.563711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n",
      "100%|██████████| 83.3M/83.3M [00:00<00:00, 210MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation\n",
    "train_df, val_df = train_test_split(train_df, stratify=train_df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_ds = SoilDataset(train_df, train_dir, transform=transform_train)\n",
    "val_ds = SoilDataset(val_df, train_dir, transform=transform_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load ResNet34 with pretrained weights\n",
    "weights = ResNet34_Weights.DEFAULT\n",
    "model = resnet34(weights=weights)\n",
    "\n",
    "# Replace final classification layer to match 4 soil classes\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "\n",
    "# Move model to GPU/CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss, optimizer, and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f29b18-bdfa-4d61-9deb-d30ac6695e93",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "- Defined a `train_model` function that trains the model for a specified number of epochs.\n",
    "- In each epoch:\n",
    "  - Set the model to training mode and computed the training loss.\n",
    "  - Performed backpropagation and optimizer steps.\n",
    "  - Updated the learning rate scheduler.\n",
    "- After each epoch, switched to evaluation mode:\n",
    "  - Predicted on the validation set.\n",
    "  - Calculated per-class F1 scores to assess performance on each soil type.\n",
    "  - Printed training loss and validation F1 scores for monitoring.\n",
    "- Trained the model for 15 epochs.\n",
    "- Saved the final trained model weights to `best_model.pth` for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae7843a7-8468-49c5-a76e-6ad1beb65de7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T11:24:07.076451Z",
     "iopub.status.busy": "2025-05-23T11:24:07.075971Z",
     "iopub.status.idle": "2025-05-23T11:28:28.639122Z",
     "shell.execute_reply": "2025-05-23T11:28:28.638485Z",
     "shell.execute_reply.started": "2025-05-23T11:24:07.076428Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/15\n",
      "  ➤ Train Loss: 0.4024\n",
      "  ➤ Per-class F1: [0.95327103 0.91489362 0.94736842 0.98039216]\n",
      "  ➤ Min F1: 0.9149\n",
      "\n",
      "Epoch 2/15\n",
      "  ➤ Train Loss: 0.1459\n",
      "  ➤ Per-class F1: [0.96650718 0.95652174 0.93975904 0.98039216]\n",
      "  ➤ Min F1: 0.9398\n",
      "\n",
      "Epoch 3/15\n",
      "  ➤ Train Loss: 0.1204\n",
      "  ➤ Per-class F1: [0.94339623 0.9375     0.91358025 0.92783505]\n",
      "  ➤ Min F1: 0.9136\n",
      "\n",
      "Epoch 4/15\n",
      "  ➤ Train Loss: 0.0828\n",
      "  ➤ Per-class F1: [0.94       0.96774194 0.87640449 0.98076923]\n",
      "  ➤ Min F1: 0.8764\n",
      "\n",
      "Epoch 5/15\n",
      "  ➤ Train Loss: 0.0607\n",
      "  ➤ Per-class F1: [0.96039604 0.94845361 0.94117647 0.98039216]\n",
      "  ➤ Min F1: 0.9412\n",
      "\n",
      "Epoch 6/15\n",
      "  ➤ Train Loss: 0.0469\n",
      "  ➤ Per-class F1: [0.98076923 0.9787234  0.96296296 0.99029126]\n",
      "  ➤ Min F1: 0.9630\n",
      "\n",
      "Epoch 7/15\n",
      "  ➤ Train Loss: 0.0318\n",
      "  ➤ Per-class F1: [0.98550725 0.98924731 0.97560976 1.        ]\n",
      "  ➤ Min F1: 0.9756\n",
      "\n",
      "Epoch 8/15\n",
      "  ➤ Train Loss: 0.0282\n",
      "  ➤ Per-class F1: [0.98550725 0.98924731 0.97560976 1.        ]\n",
      "  ➤ Min F1: 0.9756\n",
      "\n",
      "Epoch 9/15\n",
      "  ➤ Train Loss: 0.0353\n",
      "  ➤ Per-class F1: [0.97584541 0.9787234  0.96296296 1.        ]\n",
      "  ➤ Min F1: 0.9630\n",
      "\n",
      "Epoch 10/15\n",
      "  ➤ Train Loss: 0.0532\n",
      "  ➤ Per-class F1: [0.98076923 0.97826087 0.97560976 1.        ]\n",
      "  ➤ Min F1: 0.9756\n",
      "\n",
      "Epoch 11/15\n",
      "  ➤ Train Loss: 0.0234\n",
      "  ➤ Per-class F1: [0.98058252 0.9787234  0.97560976 1.        ]\n",
      "  ➤ Min F1: 0.9756\n",
      "\n",
      "Epoch 12/15\n",
      "  ➤ Train Loss: 0.0202\n",
      "  ➤ Per-class F1: [0.98550725 0.98924731 0.97560976 1.        ]\n",
      "  ➤ Min F1: 0.9756\n",
      "\n",
      "Epoch 13/15\n",
      "  ➤ Train Loss: 0.0154\n",
      "  ➤ Per-class F1: [0.98550725 0.98924731 0.97560976 1.        ]\n",
      "  ➤ Min F1: 0.9756\n",
      "\n",
      "Epoch 14/15\n",
      "  ➤ Train Loss: 0.0201\n",
      "  ➤ Per-class F1: [0.98058252 0.96842105 0.97560976 0.99029126]\n",
      "  ➤ Min F1: 0.9684\n",
      "\n",
      "Epoch 15/15\n",
      "  ➤ Train Loss: 0.0250\n",
      "  ➤ Per-class F1: [0.97142857 0.9787234  0.93670886 0.99029126]\n",
      "  ➤ Min F1: 0.9367\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        f1 = f1_score(all_labels, all_preds, average=None)\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  ➤ Train Loss: {running_loss / len(train_loader):.4f}\")\n",
    "        print(f\"  ➤ Per-class F1: {f1}\")\n",
    "        print(f\"  ➤ Min F1: {min(f1):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, epochs=15)\n",
    "\n",
    "# Save the best trained model\n",
    "torch.save(model.state_dict(), \"best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a7756-9d47-4432-97f0-77fb399e5d7a",
   "metadata": {},
   "source": [
    "## Preparing Test Data\n",
    "\n",
    "- Copied test images from the input directory to the working directory to allow image format conversions.\n",
    "- Loaded the CSV file containing test image IDs.\n",
    "- Converted unsupported image formats (`.webp`, `.gif`) to `.jpg` for compatibility with the model.\n",
    "- Implemented a function to resolve the correct image filename considering possible extensions (`.jpg`, `.jpeg`, `.png`).\n",
    "- Applied this function to ensure all test image IDs map to valid image files, dropping any missing files to maintain data integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10331740-e732-4d1d-8379-1959fa14fd5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T11:28:46.986645Z",
     "iopub.status.busy": "2025-05-23T11:28:46.985864Z",
     "iopub.status.idle": "2025-05-23T11:28:48.902913Z",
     "shell.execute_reply": "2025-05-23T11:28:48.902124Z",
     "shell.execute_reply.started": "2025-05-23T11:28:46.986615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted img_91cbc6e5.gif → img_91cbc6e5.jpg\n",
      "Converted img_f22972ea.webp → img_f22972ea.jpg\n"
     ]
    }
   ],
   "source": [
    "# Copy test files to working directory to allow .jpg conversion\n",
    "test_dir = '/kaggle/input/soil-classification/soil_classification-2025/test'\n",
    "test_ids_path = '/kaggle/input/soil-classification/soil_classification-2025/test_ids.csv'\n",
    "working_test_dir = '/kaggle/working/test'\n",
    "\n",
    "if not os.path.exists(working_test_dir):\n",
    "    shutil.copytree(test_dir, working_test_dir)\n",
    "\n",
    "# Load test image IDs\n",
    "test_df = pd.read_csv(test_ids_path)\n",
    "\n",
    "# Handle .webp/.gif files → .jpg\n",
    "test_files = os.listdir(working_test_dir)\n",
    "test_files_lower = {f.lower(): f for f in test_files}\n",
    "\n",
    "for fname in test_files:\n",
    "    ext = os.path.splitext(fname)[1].lower()\n",
    "    if ext in ['.webp', '.gif']:\n",
    "        base = os.path.splitext(fname)[0]\n",
    "        new_path = os.path.join(working_test_dir, base + \".jpg\")\n",
    "        if not os.path.exists(new_path):\n",
    "            try:\n",
    "                img = Image.open(os.path.join(working_test_dir, fname)).convert(\"RGB\")\n",
    "                img.save(new_path, format=\"JPEG\")\n",
    "                print(f\"Converted {fname} → {base}.jpg\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to convert {fname}: {e}\")\n",
    "\n",
    "# Resolve file paths in test folder\n",
    "def resolve_file(image_id):\n",
    "    base = os.path.splitext(image_id)[0]\n",
    "    for ext in ['.jpg', '.jpeg', '.png']:\n",
    "        fname = f\"{base}{ext}\"\n",
    "        if fname.lower() in test_files_lower:\n",
    "            return test_files_lower[fname.lower()]\n",
    "        full_path = os.path.join(working_test_dir, fname)\n",
    "        if os.path.exists(full_path):\n",
    "            return fname\n",
    "    return None\n",
    "\n",
    "# Apply resolution\n",
    "test_df['resolved_image'] = test_df['image_id'].apply(resolve_file)\n",
    "test_df = test_df.dropna(subset=['resolved_image']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9cefc-a706-441d-83b7-52874dfeab66",
   "metadata": {},
   "source": [
    "## Test Dataset and DataLoader for Inference\n",
    "\n",
    "- Defined the test transformation pipeline, which is the same as the validation transforms (resize + normalization).\n",
    "- Created a custom `TestDataset` class to:\n",
    "  - Load and transform test images.\n",
    "  - Return the transformed image along with its original image ID.\n",
    "- Initialized the test dataset and corresponding dataloader for batch inference with a batch size of 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee21f7aa-a666-460e-9307-e68c5ec4c149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T11:30:52.162751Z",
     "iopub.status.busy": "2025-05-23T11:30:52.162457Z",
     "iopub.status.idle": "2025-05-23T11:30:52.168757Z",
     "shell.execute_reply": "2025-05-23T11:30:52.167991Z",
     "shell.execute_reply.started": "2025-05-23T11:30:52.162729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Final test transform (same as val)\n",
    "test_transform = transform_val\n",
    "\n",
    "# Create dataset for test inference\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        path = os.path.join(self.img_dir, row['resolved_image'])\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image, row['image_id']\n",
    "\n",
    "test_dataset = TestDataset(test_df, working_test_dir, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2cf6d-5dac-4fe2-afa2-a9bfff7a9c95",
   "metadata": {},
   "source": [
    "## Model Inference on Test Data\n",
    "\n",
    "- Reloaded the pretrained ResNet34 model architecture and replaced the final layer to match the 4 soil classes.\n",
    "- Loaded the saved trained weights (`best_model.pth`) into the model.\n",
    "- Set the model to evaluation mode and moved it to the appropriate device (GPU/CPU).\n",
    "- Created a reverse mapping from numeric labels back to soil type names.\n",
    "- Ran inference on the test dataset:\n",
    "  - Predicted the soil class for each test image.\n",
    "  - Stored the predictions along with their corresponding image IDs for submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48fc1045-c16b-499b-a080-402f590d7e0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T11:30:57.134076Z",
     "iopub.status.busy": "2025-05-23T11:30:57.133820Z",
     "iopub.status.idle": "2025-05-23T11:30:59.997677Z",
     "shell.execute_reply": "2025-05-23T11:30:59.996888Z",
     "shell.execute_reply.started": "2025-05-23T11:30:57.134056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load trained model weights\n",
    "model = resnet34(weights=ResNet34_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Label decoding map\n",
    "inv_label_map = {0: 'Alluvial soil', 1: 'Black Soil', 2: 'Clay soil', 3: 'Red soil'}\n",
    "\n",
    "# Predict test labels\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for images, image_ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        for image_id, pred in zip(image_ids, preds):\n",
    "            predictions.append((image_id, inv_label_map[pred]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a4f03f-cf1d-46e7-a089-8262781c0d67",
   "metadata": {},
   "source": [
    "## Creating Submission File\n",
    "\n",
    "- Created a DataFrame from the list of predictions containing image IDs and their predicted soil types.\n",
    "- Saved the DataFrame to a CSV file named `submission.csv` without the index column.\n",
    "- Printed confirmation showing the number of rows generated in the submission file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b18603-3675-4503-918a-3d2d1fc8b93b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-23T11:31:07.878873Z",
     "iopub.status.busy": "2025-05-23T11:31:07.878509Z",
     "iopub.status.idle": "2025-05-23T11:31:07.889343Z",
     "shell.execute_reply": "2025-05-23T11:31:07.888649Z",
     "shell.execute_reply.started": "2025-05-23T11:31:07.878843Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ submission.csv generated with 341 rows.\n"
     ]
    }
   ],
   "source": [
    "# Create submission DataFrame and save to CSV\n",
    "submission = pd.DataFrame(predictions, columns=[\"image_id\", \"soil_type\"])\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"submission.csv generated with\", len(submission), \"rows.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12375409,
     "sourceId": 102672,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
