{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae31d38",
   "metadata": {},
   "source": [
    "# ðŸ§  Soil Classification - Training Notebook\n",
    "Welcome to the training notebook for our soil classification challenge. In this notebook, we:\n",
    "- Load labeled soil + synthetic data\n",
    "- Train a classifier to detect \"soil\" vs \"not soil\"\n",
    "- Track F1 score\n",
    "- Save the best model\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43720ec4",
   "metadata": {},
   "source": [
    "We'll split this into cells as they would appear in a notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f19981",
   "metadata": {},
   "source": [
    "## Cell 1: Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef00c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from src.preprocessing import SoilDataset, TestSoilDataset, get_transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd7ab1",
   "metadata": {},
   "source": [
    "Cell 2: Paths & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1034b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = './data/soil_competition-2025'\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
    "TEST_DIR = os.path.join(BASE_DIR, 'test')\n",
    "LABEL_FILE = os.path.join(BASE_DIR, 'train_labels.csv')\n",
    "\n",
    "df = pd.read_csv(LABEL_FILE)\n",
    "\n",
    "# Stratified split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bade969",
   "metadata": {},
   "source": [
    " Cell 3: Transforms and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9600d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda (NVIDIA GeForce RTX 4050 Laptop GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "train_transform, val_transform = get_transforms()\n",
    "\n",
    "train_dataset = SoilDataset(train_df, TRAIN_DIR, train_transform)\n",
    "val_dataset = SoilDataset(val_df, TRAIN_DIR, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f8fbb0",
   "metadata": {},
   "source": [
    " cuda setup and check , working on laptop vs code locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db61fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU device name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e7e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Output shape: torch.Size([8, 1000])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd414db",
   "metadata": {},
   "source": [
    "Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c33b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "epochs = 10\n",
    "patience = 2\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            probs = torch.argmax(outputs, dim=1)\n",
    "            preds.extend(probs.cpu().numpy())\n",
    "            targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(targets, preds)\n",
    "    print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, Val F1={f1:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if f1 > best_f1:\n",
    "        torch.save(model.state_dict(), \"../data/model_best.pth\")\n",
    "        best_f1 = f1\n",
    "        wait = 0\n",
    "        print(\"âœ… Best model saved!\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"ðŸ›‘ Early stopping.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb2030",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8281b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_probs = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                all_probs.extend(outputs.cpu().numpy().flatten())\n",
    "                all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        all_probs = np.array(all_probs)\n",
    "        all_targets = np.array(all_targets)\n",
    "\n",
    "        # Threshold tuning\n",
    "        thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "        f1_scores = []\n",
    "\n",
    "        for t in thresholds:\n",
    "            preds = (all_probs > t).astype(int)\n",
    "            f1 = f1_score(all_targets, preds)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        max_f1 = max(f1_scores)\n",
    "        max_thresh = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "        if max_f1 > best_f1:\n",
    "            best_f1 = max_f1\n",
    "            best_threshold = max_thresh\n",
    "            torch.save(model.state_dict(), './model_best.pth')  # Save best model\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val F1: {max_f1:.4f} at threshold {max_thresh:.2f}\")\n",
    "\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d970b6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torchviz\n",
      "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchviz) (2.5.1+cu121)\n",
      "Requirement already satisfied: graphviz in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.5)\n",
      "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
      "Installing collected packages: torchviz\n",
      "Successfully installed torchviz-0.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ymongo (c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ymongo (c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ymongo (c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torchviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d30abb",
   "metadata": {},
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = train_model(model, train_loader, val_loader, epochs=10)\n",
    "print(f\"Best threshold selected: {best_threshold}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Soil CUDA)",
   "language": "python",
   "name": "soil-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
