{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7673ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Author: sanskar khandelwal\n",
    "Team Name: TheLastTransformer\n",
    "Team Members: sanskar khandelwal \n",
    "Leaderboard Rank: 56\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52433223",
   "metadata": {},
   "source": [
    "# Cell 1: Notebook Metadata\n",
    "This cell specifies author information, team details, and leaderboard rank for documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae31d38",
   "metadata": {},
   "source": [
    "#  Soil Classification - Training Notebook\n",
    "Welcome to the training notebook for our soil classification challenge. In this notebook, we:\n",
    "-  labeled soil + resnet18 + one shot svm \n",
    "- Train a classifier to detect \"soil\" vs \"not soil\"\n",
    "- Track F1 score\n",
    "- Save the best model\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7efb06c",
   "metadata": {},
   "source": [
    "# Cell : Step 1 - Import Libraries Section\n",
    "Provide a section header indicating the start of library imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef00c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1034b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ðŸ–¼ï¸ Step 2: Image Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a7e50",
   "metadata": {},
   "source": [
    "# Cell 6: Step 2 - Image Transformations\n",
    "Define the image preprocessing pipeline including resizing, tensor conversion, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9600d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda (NVIDIA GeForce RTX 4050 Laptop GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "train_transform, val_transform = get_transforms()\n",
    "\n",
    "train_dataset = SoilDataset(train_df, TRAIN_DIR, train_transform)\n",
    "val_dataset = SoilDataset(val_df, TRAIN_DIR, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602e791",
   "metadata": {},
   "source": [
    "# Cell 8: Create Datasets and DataLoaders\n",
    "Instantiate `SoilDataset` objects and wrap them in DataLoaders with batching and shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a5607",
   "metadata": {},
   "source": [
    "# Cell 9: CUDA Setup Note\n",
    "Describe checking for GPU availability and local VS Code configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db61fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "GPU device name: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3926d21",
   "metadata": {},
   "source": [
    "# Cell 10: Model Initialization\n",
    "Set device to GPU/CPU, load pretrained ResNet18, modify final layer, and move model to device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e7e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU Device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "Output shape: torch.Size([8, 1000])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f4288",
   "metadata": {},
   "source": [
    "# Cell 11: Duplicate Model Setup\n",
    "Repeat device and model initialization (likely redundant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ðŸ“‚ Step 4: Paths and CSVs\n",
    "train_csv = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train_labels.csv'\n",
    "test_csv = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test_ids.csv'\n",
    "train_dir = '/kaggle/input/soil-classification-part-2/soil_competition-2025/train'\n",
    "test_dir = '/kaggle/input/soil-classification-part-2/soil_competition-2025/test'\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d4b24",
   "metadata": {},
   "source": [
    "# Cell 12: Step 4 - Paths and CSV Loading\n",
    "Define dataset paths and load training and test CSV metadata into DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd414db",
   "metadata": {},
   "source": [
    "Loss & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5158c7ea",
   "metadata": {},
   "source": [
    "# Cell 13: Loss & Optimizer Section Header\n",
    "Introduce the configuration of loss function and optimizer for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c33b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ðŸ§º Step 5: Custom Dataset\n",
    "class SoilDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform):\n",
    "        self.df = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.df.iloc[idx]['image_id']\n",
    "        image_path = os.path.join(self.img_dir, image_id)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image, image_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facdb778",
   "metadata": {},
   "source": [
    "# Cell 14: Custom Dataset Class\n",
    "Implement `SoilDataset` to load images, apply transforms, and return tensors with IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb2030",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97cab3",
   "metadata": {},
   "source": [
    "# Cell 15: Training Function Header\n",
    "Section header for defining the model training loop function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8281b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_probs = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                all_probs.extend(outputs.cpu().numpy().flatten())\n",
    "                all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "        all_probs = np.array(all_probs)\n",
    "        all_targets = np.array(all_targets)\n",
    "\n",
    "        # Threshold tuning\n",
    "        thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "        f1_scores = []\n",
    "\n",
    "        for t in thresholds:\n",
    "            preds = (all_probs > t).astype(int)\n",
    "            f1 = f1_score(all_targets, preds)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        max_f1 = max(f1_scores)\n",
    "        max_thresh = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "        if max_f1 > best_f1:\n",
    "            best_f1 = max_f1\n",
    "            best_threshold = max_thresh\n",
    "            torch.save(model.state_dict(), './model_best.pth')  # Save best model\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val F1: {max_f1:.4f} at threshold {max_thresh:.2f}\")\n",
    "\n",
    "    return best_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d61a5be",
   "metadata": {},
   "source": [
    "# Cell 16: Define Training Loop\n",
    "Implement `train_model` to handle training epochs, validation, threshold tuning, and checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0dcea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(SoilDataset(train_df, train_dir, transform), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(SoilDataset(test_df, test_dir, transform), batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfcf8d6",
   "metadata": {},
   "source": [
    "# Cell 17: Initialize DataLoaders for Feature Extraction\n",
    "Create DataLoaders specifically for the feature extraction phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ðŸ” Step 6: Feature Extraction\n",
    "def extract_features(dataloader, model, device):\n",
    "    features = []\n",
    "    ids = []\n",
    "    with torch.no_grad():\n",
    "        for images, image_ids in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images = images.to(device)\n",
    "            feats = model(images).cpu().numpy()\n",
    "            features.append(feats)\n",
    "            ids.extend(image_ids)\n",
    "    return np.vstack(features), ids\n",
    "\n",
    "train_features, _ = extract_features(train_loader, resnet, device)\n",
    "test_features, test_ids = extract_features(test_loader, resnet, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad8243",
   "metadata": {},
   "source": [
    "# Cell 18: Step 6 - Feature Extraction\n",
    "Define and apply feature extraction using the trained model and DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d970b6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torchviz\n",
      "  Downloading torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchviz) (2.5.1+cu121)\n",
      "Requirement already satisfied: graphviz in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (4.13.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->torchviz) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.5)\n",
      "Downloading torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
      "Installing collected packages: torchviz\n",
      "Successfully installed torchviz-0.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ymongo (c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ymongo (c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ymongo (c:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torchviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146e8e86",
   "metadata": {},
   "source": [
    "# Cell 19: Install Additional Dependencies\n",
    "Install packages like `torchviz` required for model visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d30abb",
   "metadata": {},
   "source": [
    "### ðŸ’¾ Step 7: Save Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d410d7",
   "metadata": {},
   "source": [
    "# Cell 20: Step 7 - Save Features Section\n",
    "Section header for saving extracted feature arrays to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(\"train_features.npy\", train_features)\n",
    "np.save(\"test_features.npy\", test_features)\n",
    "np.save(\"test_ids.npy\", test_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e42667",
   "metadata": {},
   "source": [
    "# Cell 21: Save Extracted Features\n",
    "Save NumPy arrays of train features, test features, and IDs for downstream tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Soil CUDA)",
   "language": "python",
   "name": "soil-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
